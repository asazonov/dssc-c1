#formalised estimation of genes (no spike)
df = dim(counts.matrix)[2]-1
psia1theta = mean(1/size_factors) + a1
minBiolDisp = .5 ^ 2
cv2th = a0 + minBiolDisp + a0 * minBiolDisp
testDenom = ( counts.avg * psia1theta + counts.avg^2 * cv2th ) / ( 1 + cv2th/df )
p = 1 - pchisq( counts.var * (df-1) / testDenom, df-1 )
padj = p.adjust(p, "BH")
sig = padj<0.1
high.var = names(sig)[which(sig)]
x_vals = 10^seq(from = -3, to = 5, length.out=1000)
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
counts.sig = counts.adj[which(rownames(counts.matrix)%in%high.var), ]
write.csv(counts.cv2, file = 'vargenes_y.csv')
write.csv(counts.avg, file = 'vargenes_x.csv')
write.csv(as.numeric(sig), file = 'vargenes_signif.csv')
write.csv(rbind(counts.cv2, count.avg, as.numeric(sig)), "vargenes_all.csv")
#step 3: clustering and identification
spearman = cor(counts.sig); spearman = (-spearman+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
cluster = hclust(as.dist(spearman), method = 'average')
clust.labels = cutreeDynamic(cluster, method = "hybrid", minClusterSize = 10, deepSplit = 0, distM = spearman)
summary(clust.labels)
hist(clust.labels, breaks = length(unique(clust.labels)))
plot(cluster, labels = clust.labels)
write.csv(clust.labels, "cluster_labels.csv")
#step 4: try dimension reduction steps: PCA/t-SNE
tsne = Rtsne(X=t(counts.sig), verbose = T, initial_dims = dim(counts.sig)[1])
plot(tsne$Y, col = clust.labels)
write.csv(tsne$Y, "tsne_coords.csv")
pca = prcomp(t(counts.sig))
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
write.csv(pca$x, "princomps.csv")
dm = diffuse(dist(t(counts.sig)))
plot(dm, color = clust.labels)
#####
#BASiCS approach
?setwd
getwd()
setwd(out_loc)
out_loc = "~/Dropbox/Work/hackathon/data_out"
setwd(out_loc)
file_loc = "~/Dropbox/Work/hackathon/"
out_loc = "~/Dropbox/Work/hackathon/data_out"
setwd(out_loc)
out_loc = "~/Dropbox/Work/hackathon/data_out/"
setwd(out_loc)
out_loc = "~/Dropbox/Work/hackathon/dssc-c1/data_out/"
setwd(out_loc)
source("https://bioconductor.org/biocLite.R")
library(useful)
#library(devtools)
library(DESeq)
library(dynamicTreeCut)
library(Rtsne)
library(diffusionMap)
#library(BASiCS)
cellcycle = read.csv(paste0(file_loc, "dssc-c1/cellcycle.csv"), row.names = 1)
#bertie = data.frame(t(read.csv(paste0(file_loc, dssc-c1/bertie.csv", row.names = 1))) doesn't really work with the method
zebra = read.table(sep = "\t", file = paste0(file_loc, "dssc-c1/zebrafish.txt"))
ear = read.table(sep = '\t', file = paste0(file_loc, "dssc-c1/ear.txt"), header = T, row.names = 1)
mouse = read.table(sep = "\t", file = paste0(file_loc, "mouse.txt"))
brain = read.csv(file = paste0(file_loc, "brain.csv"), header = T, row.names = 1)
data = round(ear) #### MAKE THIS POINT TO THE UPLOADED FILE
data[1:5, 1:5] # as a check - put this on the page
#step 1: size factors via DESeq
counts = newCountDataSet(data, conditions = names(data))
counts = estimateSizeFactors(counts)
counts.matrix = counts(counts)
#Put the counts on the common scale if it made sizefactors:
if(!is.na(sizeFactors(counts)[1])){
counts.adj = t(t(counts.matrix)/sizeFactors(counts))
} else {counts.adj = t(t(counts.matrix))}
#step 2: high var genes (dimension reduction)
coef_var2 = function(vec_gene){
return(var(vec_gene)/(mean(vec_gene)^2))
}
counts.cv2 = apply(counts.adj, MARGIN = 1, FUN = coef_var2)
counts.avg = apply(counts.adj, MARGIN = 1, FUN = mean)
counts.var = apply(counts.adj, MARGIN = 1, FUN = var)
#in the paper they removed these low ones from the regression
lower_limit = quantile(counts.avg)[2]
cv2.fit = counts.cv2[which(counts.avg > max(0.5, lower_limit))]
avg.fit = counts.avg[which(counts.avg > max(0.5, lower_limit))]
if(is.na(sizeFactors(counts)[1])){size_factors = rep(1, dim(counts.matrix)[2])} else {size_factors = sizeFactors(counts)}
#data fit using gamma family:
model = glm(cv2.fit ~ I(1/avg.fit), family = Gamma(link = 'identity'))# native (slower) implementation
grad = model$coefficients[2]; a1 = grad
int = model$coefficients[1]; a0 = int
#formalised estimation of genes (no spike)
df = dim(counts.matrix)[2]-1
psia1theta = mean(1/size_factors) + a1
minBiolDisp = .5 ^ 2
cv2th = a0 + minBiolDisp + a0 * minBiolDisp
testDenom = ( counts.avg * psia1theta + counts.avg^2 * cv2th ) / ( 1 + cv2th/df )
p = 1 - pchisq( counts.var * (df-1) / testDenom, df-1 )
padj = p.adjust(p, "BH")
sig = padj<0.1
high.var = names(sig)[which(sig)]
x_vals = 10^seq(from = -3, to = 5, length.out=1000)
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
counts.sig = counts.adj[which(rownames(counts.matrix)%in%high.var), ]
write.csv(counts.cv2, file = 'vargenes_y.csv')
write.csv(counts.avg, file = 'vargenes_x.csv')
write.csv(as.numeric(sig), file = 'vargenes_signif.csv')
write.csv(rbind(counts.cv2, count.avg, as.numeric(sig)), "vargenes_all.csv")
#step 3: clustering and identification
spearman = cor(counts.sig); spearman = (-spearman+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
cluster = hclust(as.dist(spearman), method = 'average')
clust.labels = cutreeDynamic(cluster, method = "hybrid", minClusterSize = 10, deepSplit = 0, distM = spearman)
summary(clust.labels)
hist(clust.labels, breaks = length(unique(clust.labels)))
plot(cluster, labels = clust.labels)
write.csv(clust.labels, "cluster_labels.csv")
#step 4: try dimension reduction steps: PCA/t-SNE
tsne = Rtsne(X=t(counts.sig), verbose = T, initial_dims = dim(counts.sig)[1])
plot(tsne$Y, col = clust.labels)
write.csv(tsne$Y, "tsne_coords.csv")
pca = prcomp(t(counts.sig))
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
write.csv(pca$x, "princomps.csv")
dm = diffuse(dist(t(counts.sig)))
plot(dm, color = clust.labels)
#####
#BASiCS approach
write.csv(rbind(counts.cv2, counts.avg, as.numeric(sig)), "vargenes_all.csv")
names(clust.labels) = names(counts.sig)
corner(clust.labels)
master_record = rbind(counts.cv2, counts.avg, as.numeric(sig), clust.labels, tsne$Y)
master_record = rbind(counts.cv2, counts.avg, as.numeric(sig), clust.labels)
master_record = rbind(counts.cv2, counts.avg, as.numeric(sig))
corner(master_record)
master_record[4,] = clust.labels
rownames(vargene_record)[3] = "signif_var"
vargene_record = rbind(counts.cv2, counts.avg, as.numeric(sig))
rownames(vargene_record)[3] = "signif_var"
write.csv(vargene_record, "vargenes_all.csv")
corner(vargene_record)
?as.integer
vargene_record = rbind(counts.cv2, counts.avg, as.integer(sig))
rownames(vargene_record)[3] = "signif_var"
corner(vargene_record)
corner(clust.labels)
rownames(clust.labels) = names(counts.sig)
corner(clust.labels)
plot(tsne$Y, col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
head(tsne$Y)
class(tsne$Y)
?data.grame
?data.frame
head(data.frame(tsne$Y, names=c('x', 'y')))
head(data.frame(tsne$Y))
tsne.df = data.frame(tsne$Y)
tsne.df = data.frame(c('x', 'y')=tsne$Y)
tsne.df = data.frame(x=tsne$Y[,1], y= tsne$y[,2])
tsne.df = data.frame(x=tsne$Y, y= tsne$y)
tsne.df = data.frame(x=tsne$Y[,1], y= tsne$Y[,2])
head(tsne.df)
tsne.df = data.frame(tsne-x=tsne$Y[,1], tsne-y= tsne$Y[,2], labels = names(counts.matrix))
tsne.df = data.frame(tsne-x=tsne$Y[,1], tsne-y= tsne$Y[,2], labs = names(counts.matrix))
tsne.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], labs = names(counts.matrix))
dim(counts_matrix)
dim(counts.matrix)
tsne.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2])
dim(tsne.df)
length(names(counts.matrix))
length(rownames(counts.matrix))
length(colnames(counts.matrix))
tsne.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], labs = colnames(counts.matrix))
tsne.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], row.names = colnames(counts.matrix))
tsne.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], row.names = colnames(counts.matrix), cluster = clust.labels)
hea(tsne.df)
head(tsne.df)
data = round(ear) #### MAKE THIS POINT TO THE UPLOADED FILE
data[1:5, 1:5] # as a check - put this on the page
#step 1: size factors via DESeq
counts = newCountDataSet(data, conditions = names(data))
counts = estimateSizeFactors(counts)
counts.matrix = counts(counts)
#Put the counts on the common scale if it made sizefactors:
if(!is.na(sizeFactors(counts)[1])){
counts.adj = t(t(counts.matrix)/sizeFactors(counts))
} else {counts.adj = t(t(counts.matrix))}
#step 2: high var genes (dimension reduction)
coef_var2 = function(vec_gene){
return(var(vec_gene)/(mean(vec_gene)^2))
}
counts.cv2 = apply(counts.adj, MARGIN = 1, FUN = coef_var2)
counts.avg = apply(counts.adj, MARGIN = 1, FUN = mean)
counts.var = apply(counts.adj, MARGIN = 1, FUN = var)
#in the paper they removed these low ones from the regression
lower_limit = quantile(counts.avg)[2]
cv2.fit = counts.cv2[which(counts.avg > max(0.5, lower_limit))]
avg.fit = counts.avg[which(counts.avg > max(0.5, lower_limit))]
if(is.na(sizeFactors(counts)[1])){size_factors = rep(1, dim(counts.matrix)[2])} else {size_factors = sizeFactors(counts)}
#data fit using gamma family:
model = glm(cv2.fit ~ I(1/avg.fit), family = Gamma(link = 'identity'))# native (slower) implementation
grad = model$coefficients[2]; a1 = grad
int = model$coefficients[1]; a0 = int
#formalised estimation of genes (no spike)
df = dim(counts.matrix)[2]-1
psia1theta = mean(1/size_factors) + a1
minBiolDisp = .5 ^ 2
cv2th = a0 + minBiolDisp + a0 * minBiolDisp
testDenom = ( counts.avg * psia1theta + counts.avg^2 * cv2th ) / ( 1 + cv2th/df )
p = 1 - pchisq( counts.var * (df-1) / testDenom, df-1 )
padj = p.adjust(p, "BH")
sig = padj<0.1
high.var = names(sig)[which(sig)]
x_vals = 10^seq(from = -3, to = 5, length.out=1000)
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
counts.sig = counts.adj[which(rownames(counts.matrix)%in%high.var), ]
vargene_record = rbind(counts.cv2, counts.avg, as.integer(sig))
rownames(vargene_record)[3] = "signif_var"
write.csv(vargene_record, "vargenes.csv")
#step 3: clustering and identification
spearman = cor(counts.sig); spearman = (-spearman+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
cluster = hclust(as.dist(spearman), method = 'average')
clust.labels = cutreeDynamic(cluster, method = "hybrid", minClusterSize = 10, deepSplit = 0, distM = spearman)
summary(clust.labels)
rownames(clust.labels) = names(counts.sig)
hist(clust.labels, breaks = length(unique(clust.labels)))
plot(cluster, labels = clust.labels)
#step 4: try dimension reduction steps: PCA/t-SNE
tsne = Rtsne(X=t(counts.sig), verbose = T, initial_dims = dim(counts.sig)[1])
plot(tsne$Y, col = clust.labels)
clust.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], row.names = colnames(counts.matrix), cluster = clust.labels)
write.csv(clust.df, "clust.csv")
pca = prcomp(t(counts.sig))
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
write.csv(pca$x, "princomps.csv")
dm = diffuse(dist(t(counts.sig)))
plot(dm, color = clust.labels)
#####
#BASiCS approach
setwd(out_loc)
getwd()
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
data = round(ear) #### MAKE THIS POINT TO THE UPLOADED FILE
data[1:5, 1:5] # as a check - put this on the page
#step 1: size factors via DESeq
counts = newCountDataSet(data, conditions = names(data))
counts = estimateSizeFactors(counts)
counts.matrix = counts(counts)
#Put the counts on the common scale if it made sizefactors:
if(!is.na(sizeFactors(counts)[1])){
counts.adj = t(t(counts.matrix)/sizeFactors(counts))
} else {counts.adj = t(t(counts.matrix))}
#step 2: high var genes (dimension reduction)
coef_var2 = function(vec_gene){
return(var(vec_gene)/(mean(vec_gene)^2))
}
counts.cv2 = apply(counts.adj, MARGIN = 1, FUN = coef_var2)
counts.avg = apply(counts.adj, MARGIN = 1, FUN = mean)
counts.var = apply(counts.adj, MARGIN = 1, FUN = var)
#in the paper they removed these low ones from the regression
lower_limit = quantile(counts.avg)[2]
cv2.fit = counts.cv2[which(counts.avg > max(0.5, lower_limit))]
avg.fit = counts.avg[which(counts.avg > max(0.5, lower_limit))]
if(is.na(sizeFactors(counts)[1])){size_factors = rep(1, dim(counts.matrix)[2])} else {size_factors = sizeFactors(counts)}
#data fit using gamma family:
model = glm(cv2.fit ~ I(1/avg.fit), family = Gamma(link = 'identity'))# native (slower) implementation
grad = model$coefficients[2]; a1 = grad
int = model$coefficients[1]; a0 = int
#formalised estimation of genes (no spike)
df = dim(counts.matrix)[2]-1
psia1theta = mean(1/size_factors) + a1
minBiolDisp = .5 ^ 2
cv2th = a0 + minBiolDisp + a0 * minBiolDisp
testDenom = ( counts.avg * psia1theta + counts.avg^2 * cv2th ) / ( 1 + cv2th/df )
p = 1 - pchisq( counts.var * (df-1) / testDenom, df-1 )
padj = p.adjust(p, "BH")
sig = padj<0.1
high.var = names(sig)[which(sig)]
x_vals = 10^seq(from = -3, to = 5, length.out=1000)
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
counts.sig = counts.adj[which(rownames(counts.matrix)%in%high.var), ]
vargene_record = rbind(counts.cv2, counts.avg, as.integer(sig))
rownames(vargene_record)[3] = "signif_var"
write.csv(vargene_record, "vargenes.csv")
#step 3: clustering and identification
spearman = cor(counts.sig); spearman = (-spearman+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
cluster = hclust(as.dist(spearman), method = 'average')
clust.labels = cutreeDynamic(cluster, method = "hybrid", minClusterSize = 10, deepSplit = 0, distM = spearman)
summary(clust.labels)
rownames(clust.labels) = names(counts.sig)
hist(clust.labels, breaks = length(unique(clust.labels)))
plot(cluster, labels = clust.labels)
#step 4: try dimension reduction steps: PCA/t-SNE
tsne = Rtsne(X=t(counts.sig), verbose = T, initial_dims = dim(counts.sig)[1])
plot(tsne$Y, col = clust.labels)
clust.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], row.names = colnames(counts.matrix), cluster = clust.labels)
write.csv(clust.df, "clust.csv")
pca = prcomp(t(counts.sig))
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
write.csv(pca$x, "princomps.csv")
dm = diffuse(dist(t(counts.sig)))
plot(dm, color = clust.labels)
#####
#BASiCS approach
write.csv(t(vargene_record), "vargenes.csv")
spearman.base = cor(counts.sig); spearman = (-spearman.base+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
corner(spearman.base)
write.csv(spearman.base, out_loc)
data = round(ear) #### MAKE THIS POINT TO THE UPLOADED FILE
data[1:5, 1:5] # as a check - put this on the page
#step 1: size factors via DESeq
counts = newCountDataSet(data, conditions = names(data))
counts = estimateSizeFactors(counts)
counts.matrix = counts(counts)
#Put the counts on the common scale if it made sizefactors:
if(!is.na(sizeFactors(counts)[1])){
counts.adj = t(t(counts.matrix)/sizeFactors(counts))
} else {counts.adj = t(t(counts.matrix))}
#step 2: high var genes (dimension reduction)
coef_var2 = function(vec_gene){
return(var(vec_gene)/(mean(vec_gene)^2))
}
counts.cv2 = apply(counts.adj, MARGIN = 1, FUN = coef_var2)
counts.avg = apply(counts.adj, MARGIN = 1, FUN = mean)
counts.var = apply(counts.adj, MARGIN = 1, FUN = var)
#in the paper they removed these low ones from the regression
lower_limit = quantile(counts.avg)[2]
cv2.fit = counts.cv2[which(counts.avg > max(0.5, lower_limit))]
avg.fit = counts.avg[which(counts.avg > max(0.5, lower_limit))]
if(is.na(sizeFactors(counts)[1])){size_factors = rep(1, dim(counts.matrix)[2])} else {size_factors = sizeFactors(counts)}
#data fit using gamma family:
model = glm(cv2.fit ~ I(1/avg.fit), family = Gamma(link = 'identity'))# native (slower) implementation
grad = model$coefficients[2]; a1 = grad
int = model$coefficients[1]; a0 = int
#formalised estimation of genes (no spike)
df = dim(counts.matrix)[2]-1
psia1theta = mean(1/size_factors) + a1
minBiolDisp = .5 ^ 2
cv2th = a0 + minBiolDisp + a0 * minBiolDisp
testDenom = ( counts.avg * psia1theta + counts.avg^2 * cv2th ) / ( 1 + cv2th/df )
p = 1 - pchisq( counts.var * (df-1) / testDenom, df-1 )
padj = p.adjust(p, "BH")
sig = padj<0.1
high.var = names(sig)[which(sig)]
x_vals = 10^seq(from = -3, to = 5, length.out=1000)
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
counts.sig = counts.adj[which(rownames(counts.matrix)%in%high.var), ]
vargene_record = rbind(counts.cv2, counts.avg, as.integer(sig))
rownames(vargene_record)[3] = "signif_var"
write.csv(t(vargene_record), "vargenes.csv")
#step 3: clustering and identification
spearman.base = cor(counts.sig); spearman = (-spearman.base+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
cluster = hclust(as.dist(spearman), method = 'average')
clust.labels = cutreeDynamic(cluster, method = "hybrid", minClusterSize = 10, deepSplit = 0, distM = spearman)
summary(clust.labels)
rownames(clust.labels) = names(counts.sig)
hist(clust.labels, breaks = length(unique(clust.labels)))
plot(cluster, labels = clust.labels)
write.csv(spearman.base, 'correlation.csv')
#step 4: try dimension reduction steps: PCA/t-SNE
tsne = Rtsne(X=t(counts.sig), verbose = T, initial_dims = dim(counts.sig)[1])
plot(tsne$Y, col = clust.labels)
clust.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], row.names = colnames(counts.matrix), cluster = clust.labels)
write.csv(clust.df, "clust.csv")
pca = prcomp(t(counts.sig))
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
write.csv(pca$x, "princomps.csv")
dm = diffuse(dist(t(counts.sig)))
plot(dm, color = clust.labels)
#####
#BASiCS approach
?heatmap
heatmap(spearman.base)
length(spearman.base)
biocLite("TopGO")
biocLite("topGO")
library(TopGO)
library(topGO)
head(vargene_record)
corner(vargene_record)
data = round(ear) #### MAKE THIS POINT TO THE UPLOADED FILE
data[1:5, 1:5] # as a check - put this on the page
#step 1: size factors via DESeq
counts = newCountDataSet(data, conditions = names(data))
counts = estimateSizeFactors(counts)
counts.matrix = counts(counts)
#Put the counts on the common scale if it made sizefactors:
if(!is.na(sizeFactors(counts)[1])){
counts.adj = t(t(counts.matrix)/sizeFactors(counts))
} else {counts.adj = t(t(counts.matrix))}
#step 2: high var genes (dimension reduction)
coef_var2 = function(vec_gene){
return(var(vec_gene)/(mean(vec_gene)^2))
}
counts.cv2 = apply(counts.adj, MARGIN = 1, FUN = coef_var2)
counts.avg = apply(counts.adj, MARGIN = 1, FUN = mean)
counts.var = apply(counts.adj, MARGIN = 1, FUN = var)
#in the paper they removed these low ones from the regression
lower_limit = quantile(counts.avg)[2]
cv2.fit = counts.cv2[which(counts.avg > max(0.5, lower_limit))]
avg.fit = counts.avg[which(counts.avg > max(0.5, lower_limit))]
if(is.na(sizeFactors(counts)[1])){size_factors = rep(1, dim(counts.matrix)[2])} else {size_factors = sizeFactors(counts)}
#data fit using gamma family:
model = glm(cv2.fit ~ I(1/avg.fit), family = Gamma(link = 'identity'))# native (slower) implementation
grad = model$coefficients[2]; a1 = grad
int = model$coefficients[1]; a0 = int
#formalised estimation of genes (no spike)
df = dim(counts.matrix)[2]-1
psia1theta = mean(1/size_factors) + a1
minBiolDisp = .5 ^ 2
cv2th = a0 + minBiolDisp + a0 * minBiolDisp
testDenom = ( counts.avg * psia1theta + counts.avg^2 * cv2th ) / ( 1 + cv2th/df )
p = 1 - pchisq( counts.var * (df-1) / testDenom, df-1 )
padj = p.adjust(p, "BH")
sig = padj<0.05
high.var = names(sig)[which(sig)]
x_vals = 10^seq(from = -3, to = 5, length.out=1000)
pdf("highvarplot.pdf", width = 10, height = 10)
plot(x= counts.avg, y = counts.cv2, pch='.', log='xy', col = ifelse(names(counts.avg)%in%high.var , 'red', 'grey'))
lines(x_vals, grad/x_vals + int, col = 'blue', lwd=1)
lines(x_vals, ((grad)/x_vals + int) * qchisq(.975, df)/df, col = 'red', lwd=1, lty='dashed')
lines(x_vals, ((grad)/x_vals + int) * qchisq(.025, df)/df, col = 'red', lwd=1, lty='dashed')
dev.off()
counts.sig = counts.adj[which(rownames(counts.matrix)%in%high.var), ]
#step 3: clustering and identification
spearman.base = cor(counts.sig); spearman = (-spearman.base+1)/2; spearman[which(is.na(spearman))]=0.25 #JANKY FIX!!
cluster = hclust(as.dist(spearman), method = 'average')
clust.labels = cutreeDynamic(cluster, method = "hybrid", minClusterSize = 10, deepSplit = 0, distM = spearman)
summary(clust.labels)
rownames(clust.labels) = names(counts.sig)
hist(clust.labels, breaks = length(unique(clust.labels)))
plot(cluster, labels = clust.labels)
#step 4: try dimension reduction steps: PCA/t-SNE
tsne = Rtsne(X=t(counts.sig), verbose = T, initial_dims = dim(counts.sig)[1])
plot(tsne$Y, col = clust.labels)
pca = prcomp(t(counts.sig))
plot(x=pca$x[,1], y = pca$x[,2], col = clust.labels)
plot(x=pca$x[,1], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,3], col = clust.labels)
plot(x=pca$x[,2], y = pca$x[,4], col = clust.labels)
dm = diffuse(dist(t(counts.sig)))
plot(dm, color = clust.labels)
#####
#topGO of the clusters
#####
#data saving
write.csv(pca$x, "princomps.csv")
vargene_record = rbind(counts.cv2, counts.avg, as.integer(sig))
rownames(vargene_record)[3] = "signif_var"
write.csv(t(vargene_record), "vargenes.csv")
write.csv(spearman.base, 'correlation.csv')
clust.df = data.frame(tsne_x=tsne$Y[,1], tsne_y= tsne$Y[,2], row.names = colnames(counts.matrix), cluster = clust.labels)
write.csv(clust.df, "clust.csv")
corner(vargene_record)
vargene_record[1:3, 1:10]
dim(vargene_record)
corner(counts.matrix)
vargene_record = vargene_record[, -which(is.na(vargene_record[3,]))]
dim(vargene_record)
table(is.na(clust.df$cluster))
corner(mouse)
corner(brain)
biocLite('scde')
install_github('scde')
library(devtools)
install_github('scde')
install_github('hms-dbmi/scde')
biocLite("Cairo")
